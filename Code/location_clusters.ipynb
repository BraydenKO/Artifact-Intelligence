{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for loading/processing the images  \n",
    "from tensorflow.keras.preprocessing.image import load_img \n",
    "from tensorflow.keras.preprocessing.image import img_to_array \n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
    "\n",
    "# models \n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# clustering and dimension reduction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for everything else\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_1.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_10.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_100.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_101.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_102.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_103.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_104.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_105.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_106.jpg', 'c:\\\\Users\\\\brayk\\\\Documents\\\\CMU Classes\\\\interp\\\\Contribution\\\\Code\\\\..\\\\Dataset\\\\Banpo\\\\Banpo_107.jpg']\n",
      "There are 1106 images in the dataset\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + \"\\..\\Dataset\"\n",
    "# this list holds all the image filename\n",
    "pottery = []\n",
    "\n",
    "# Define a function to recursively walk through directories\n",
    "def collect_images(directory):\n",
    "    # Use os.scandir() to list files in the current directory\n",
    "    with os.scandir(directory) as entries:\n",
    "        for entry in entries:\n",
    "            # If it's a directory, recurse into it\n",
    "            if entry.is_dir():\n",
    "                collect_images(entry.path)\n",
    "            # If it's a file, check if it's an image file\n",
    "            elif entry.is_file() and entry.name.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):\n",
    "                # Add the image file to the list\n",
    "                pottery.append(entry.path)\n",
    "\n",
    "collect_images(path)\n",
    "\n",
    "print(pottery[:10])\n",
    "n_images = len(pottery)\n",
    "print(f\"There are {n_images} images in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19()\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "def extract_features(file, model):\n",
    "    # load the image as a 224x224 array\n",
    "    img = load_img(file, target_size=(224,224))\n",
    "    # convert from 'PIL.Image.Image' to numpy array\n",
    "    img = np.array(img) \n",
    "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3) \n",
    "    # prepare image for model\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    # get the feature vector\n",
    "    features = model.predict(imgx, verbose=False)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get feature data from these images by passing them through the model (may take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "q = os.getcwd() + \"\\..\\AnalData_location\"\n",
    "# loop through each image in the dataset\n",
    "for idx,pot in enumerate(pottery):\n",
    "    if idx%100 == 0:\n",
    "        print(f\"{idx}/{n_images}\")\n",
    "    # try to extract the features and update the dictionary\n",
    "    feat = extract_features(pot,model)\n",
    "    data[pot] = feat\n",
    "          \n",
    " \n",
    "# get a list of the filenames\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "print(feat.shape)\n",
    "feat = feat.reshape(-1,4096)\n",
    "print(feat.shape)\n",
    "with open(q,'wb') as file:\n",
    "    pickle.dump(data,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load feature data from AnalData_location (quick but requires this data to already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1106, 1, 4096)\n"
     ]
    }
   ],
   "source": [
    "q = os.getcwd() + \"\\..\\AnalData_location\"\n",
    "\n",
    "with open(q,'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "# get a list of the filenames\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "print(feat.shape)\n",
    "feat = feat.reshape(-1,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "cluster_types = {\"Banpo\":0,\"Banshan\":1,\"Machang\":2,\"Majiayao\":3,\"Miaodigou\":4}\n",
    "for path in filenames:\n",
    "    filename = os.path.basename(path)\n",
    "    name = filename[:filename.index(\"_\")]\n",
    "    clusters.append(cluster_types[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components before PCA: 4096\n",
      "Components after PCA: 10\n"
     ]
    }
   ],
   "source": [
    "n_components = 10\n",
    "pca = PCA(n_components=n_components, random_state=76101)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)\n",
    "print(f\"Components before PCA: {feat.shape[1]}\")\n",
    "print(f\"Components after PCA: {pca.n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05433478\n"
     ]
    }
   ],
   "source": [
    "score = silhouette_score(x, clusters)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
